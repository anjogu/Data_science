{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FV0phBaT5k4t",
        "bPtZE32KLpXi",
        "bFaDVl_ZsGgY",
        "LdRpIN_yROrR",
        "kOPRLHovtAnc",
        "3WuptS0MRtab",
        "wsHYvvaNxKcJ",
        "Aj1YaKV_bckt",
        "O8btojb2nQPx"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjogu/Data_science/blob/building_pipelines_with_python/%5BSolution_Notebook%5D_AfterWork_Introduction_to_Data_Engineering_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Solution Notebook] AfterWork: Introduction to Data Engineering with Python"
      ],
      "metadata": {
        "id": "FruEQ4_E5jHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-requisites"
      ],
      "metadata": {
        "id": "FV0phBaT5k4t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph3OyjK95WId"
      },
      "outputs": [],
      "source": [
        "# Import pandas as pd\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Simple Data Pipeline"
      ],
      "metadata": {
        "id": "bPtZE32KLpXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the extract function\n",
        "def extract():\n",
        "    name = \"Jane\"\n",
        "    return name\n",
        "\n",
        "# Define the transform function\n",
        "def transform(data):\n",
        "    return data\n",
        "\n",
        "# Define the load function\n",
        "def load(data):\n",
        "    print('First name:', data)\n",
        "\n",
        "# Create the data pipeline\n",
        "def data_pipeline():\n",
        "    data = extract()\n",
        "    transformed_data = transform(data)\n",
        "    load(transformed_data)\n",
        "\n",
        "# Execute the data pipeline\n",
        "data_pipeline()"
      ],
      "metadata": {
        "id": "_JxnBB-wQWiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ],
      "metadata": {
        "id": "bFaDVl_ZsGgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you are tasked with creating a basic data pipeline to process and display information about a person's email address.\n",
        "\n",
        "* Extract Function: Implement extract() to return the email address \"jane.doe@example.com\".\n",
        "* Transform Function: Implement transform(data) to convert the email address data to lowercase.\n",
        "* Load Function: Implement load(data) to print \"Person's email address: \" followed by data.\n",
        "\n",
        "* Data Pipeline: Implement data_pipeline() to:\n",
        "  * Call extract() to obtain the person's email address.\n",
        "  * Call transform() with the extracted email address.\n",
        "  * Call load() to display the transformed email address.\n",
        "\n",
        "* Execute the Data Pipeline: Call data_pipeline() to execute the data pipeline."
      ],
      "metadata": {
        "id": "fFJy0QoRsLCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the extract function\n",
        "def extract():\n",
        "    return \"jane.doe@example.com\"\n",
        "\n",
        "# Define the transform function\n",
        "def transform(data):\n",
        "    return data.lower()\n",
        "\n",
        "# Define the load function\n",
        "def load(data):\n",
        "    print('Person\\'s email address:', data)\n",
        "\n",
        "# Create the data pipeline\n",
        "def data_pipeline():\n",
        "    data = extract()\n",
        "    transformed_data = transform(data)\n",
        "    load(transformed_data)\n",
        "\n",
        "# Execute the data pipeline\n",
        "data_pipeline()"
      ],
      "metadata": {
        "id": "V8O2JWoqsKme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. CSV Data Pipeline"
      ],
      "metadata": {
        "id": "LdRpIN_yROrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the extract function\n",
        "def extract():\n",
        "    employees = pd.read_csv('employees.csv')\n",
        "    departments = pd.read_csv('departments.csv')\n",
        "    return employees, departments\n",
        "\n",
        "# Define the transform function\n",
        "def transform(employees, departments):\n",
        "    merged_data = pd.merge(employees, departments, on='department_id')\n",
        "    return merged_data\n",
        "\n",
        "# Define the load function\n",
        "def load(data):\n",
        "    data.to_csv('merged_data.csv', index=False)\n",
        "\n",
        "# Create the data pipeline\n",
        "def data_pipeline():\n",
        "    employees, departments = extract()\n",
        "    merged_data = transform(employees, departments)\n",
        "    load(merged_data)\n",
        "\n",
        "# Execute the data pipeline\n",
        "data_pipeline()"
      ],
      "metadata": {
        "id": "T1yXVaHkLuei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ],
      "metadata": {
        "id": "kOPRLHovtAnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carry out ETL on the [movie_ratings.csv](https://bit.ly/3yNFJYx) dataset provided. Some actions you could carry out in the transform stage include:\n",
        "- Calculating the average rating for each movie.\n",
        "- Calculating the number of ratings per user.\n",
        "- Grouping movies into rating categories (e.g., high-rated, low-rated)."
      ],
      "metadata": {
        "id": "CPBn25ictGLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract function\n",
        "def extract():\n",
        "    patients = pd.read_csv('https://bit.ly/3yNFJYx')\n",
        "    return patients\n",
        "\n",
        "# Transform function\n",
        "def transform(patients):\n",
        "\n",
        "    # Remove missing values and duplicates\n",
        "    patients.dropna(inplace=True)\n",
        "    patients.drop_duplicates(inplace=True)\n",
        "\n",
        "    # Encode the Gender column\n",
        "    patients['Gender'] = patients['Gender'].map({'Male': 0, 'Female': 1, 'Other': 2})\n",
        "\n",
        "    # Convert Date of Birth, Admission Date, and Discharge Date to datetime. Also calculate age and create\n",
        "    patients['Date of Birth'] = pd.to_datetime(patients['Date of Birth'])\n",
        "    patients['Admission Date'] = pd.to_datetime(patients['Admission Date'])\n",
        "    patients['Discharge Date'] = pd.to_datetime(patients['Discharge Date'])\n",
        "    patients['Age'] = (patients['Admission Date'] - patients['Date of Birth']).dt.days // 365\n",
        "\n",
        "    # Calculate the length of hospital stay\n",
        "    patients['Length of Stay'] = (patients['Discharge Date'] - patients['Admission Date']).dt.days\n",
        "\n",
        "    # Extract Blood Pressure and Pulse from Vital Signs\n",
        "    patients[['Blood Pressure', 'Pulse']] = patients['Vital Signs'].str.extract(r'Blood Pressure: (\\d+/\\d+), Pulse: (\\d+)')\n",
        "    patients.drop('Vital Signs', axis=1, inplace=True)\n",
        "\n",
        "    # Aggregate data monthly and yearly\n",
        "    patients['Year'] = patients['Admission Date'].dt.year\n",
        "    patients['Month'] = patients['Admission Date'].dt.month\n",
        "\n",
        "    # Grouping\n",
        "    monthly_aggregation = patients.groupby(['Year', 'Month']).agg({\n",
        "        'Patient ID': 'count',\n",
        "        'Age': 'mean',\n",
        "        'Length of Stay': 'mean'\n",
        "    })\n",
        "    yearly_aggregation = patients.groupby(['Year']).agg({\n",
        "        'Patient ID': 'count',\n",
        "        'Age': 'mean',\n",
        "        'Length of Stay': 'mean'\n",
        "    })\n",
        "\n",
        "    return patients, monthly_aggregation, yearly_aggregation\n",
        "\n",
        "# Load function\n",
        "def load(patients, monthly_aggregation, yearly_aggregation):\n",
        "    patients.to_csv('transformed_patient_health_records.csv', index=False)\n",
        "    monthly_aggregation.to_csv('monthly_aggregation.csv')\n",
        "    yearly_aggregation.to_csv('yearly_aggregation.csv')\n",
        "\n",
        "# Data pipeline function\n",
        "def data_pipeline():\n",
        "    patients = extract()\n",
        "    patients, monthly_aggregation, yearly_aggregation = transform(patients)\n",
        "    load(patients, monthly_aggregation, yearly_aggregation)\n",
        "\n",
        "# Execute the data pipeline\n",
        "data_pipeline()"
      ],
      "metadata": {
        "id": "03fbdU5etF8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. JSON Data Pipeline"
      ],
      "metadata": {
        "id": "3WuptS0MRtab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract function (Employees: https://bit.ly/4bSEXrV, Departments: https://bit.ly/3yQaKLu)\n",
        "def extract():\n",
        "    employees = pd.read_json('https://bit.ly/4bSEXrV', orient='records')\n",
        "    departments = pd.read_json('https://bit.ly/3yQaKLu', orient='records')\n",
        "    return employees, departments\n",
        "\n",
        "# Transform function\n",
        "def transform(employees, departments):\n",
        "    # Merge employees and departments on 'department_id' and 'id'\n",
        "    merged_data = pd.merge(employees, departments, left_on='department_id', right_on='id', suffixes=('_emp', '_dept'))\n",
        "    transformed_data = merged_data[['id_emp', 'name_emp', 'age', 'gender', 'department_id', 'name_dept', 'location']]\n",
        "    transformed_data.columns = ['id', 'name', 'age', 'gender', 'department_id', 'department_name', 'department_location']\n",
        "    return transformed_data\n",
        "\n",
        "# Load function\n",
        "def load(data):\n",
        "    data.to_csv('transformed_json_data.csv', index=False)\n",
        "\n",
        "# Create the data pipeline\n",
        "def data_pipeline():\n",
        "    employees, departments = extract()\n",
        "    transformed_data = transform(employees, departments)\n",
        "    load(transformed_data)\n",
        "\n",
        "# Execute the data pipeline\n",
        "data_pipeline()"
      ],
      "metadata": {
        "id": "nnZABw7oRtad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ],
      "metadata": {
        "id": "wsHYvvaNxKcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a data pipeline for merging order and product data, calculating derived values, and saving the transformed data into a CSV file."
      ],
      "metadata": {
        "id": "9FlV5TTS1upk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract function: (Orders: https://bit.ly/4bKdvwo, Products: https://bit.ly/45eJKkN)\n",
        "def extract():\n",
        "    orders = pd.read_json('https://bit.ly/4bKdvwo', orient='records')\n",
        "    products = pd.read_json('https://bit.ly/45eJKkN', orient='records')\n",
        "    return orders, products\n",
        "\n",
        "# Transform function\n",
        "def transform(orders, products):\n",
        "    merged_data = pd.merge(orders, products, on='product_id', how='left')\n",
        "    merged_data['total_price'] = merged_data['quantity'] * merged_data['price']\n",
        "    transformed_data = merged_data[['order_id', 'product_id', 'name', 'quantity', 'price', 'total_price']]\n",
        "    transformed_data.columns = ['Order ID', 'Product ID', 'Product Name', 'Quantity', 'Price', 'Total Price']\n",
        "    return transformed_data\n",
        "\n",
        "# Load function\n",
        "def load(data):\n",
        "    data.to_csv('order_product_details.csv', index=False)\n",
        "\n",
        "# Create the data pipeline\n",
        "def data_pipeline():\n",
        "    orders, products = extract()\n",
        "    transformed_data = transform(orders, products)\n",
        "    load(transformed_data)\n",
        "\n",
        "# Execute the data pipeline\n",
        "data_pipeline()"
      ],
      "metadata": {
        "id": "PZAQb9SxxSTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Data Pipeline from Scraped data"
      ],
      "metadata": {
        "id": "Aj1YaKV_bckt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the required libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "zfHXh06XdrtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract function\n",
        "def extract():\n",
        "    # Send a GET request to the URL and parse the HTML\n",
        "    response = requests.get(\"http://books.toscrape.com/\")\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all book containers\n",
        "    books = []\n",
        "    book_containers = soup.find_all('article', class_='product_pod')\n",
        "\n",
        "    # Extract title, price_color, and star-rating from each book container\n",
        "    for container in book_containers:\n",
        "        title = container.find('h3').a['title']\n",
        "        price = container.find('p', class_='price_color').text.strip()\n",
        "        rating = container.find('p', class_='star-rating')['class'][1]\n",
        "        books.append([title, price, rating])\n",
        "\n",
        "    return books\n",
        "\n",
        "# Transform function\n",
        "def transform(data):\n",
        "    df = pd.DataFrame(data, columns=['Title', 'Price', 'Rating'])\n",
        "    df['Price'] = df['Price'].apply(lambda x: float(x.strip('Â£')))\n",
        "    df['Rating'] = pd.to_numeric(df['Rating'])\n",
        "    return df\n",
        "\n",
        "# Load function\n",
        "def load(data):\n",
        "    data.to_csv('books_data.csv', index=False)\n",
        "\n",
        "# Create the data pipeline\n",
        "def data_pipeline():\n",
        "    book_data = extract()\n",
        "    df = transform(book_data)\n",
        "    load(df)\n",
        "\n",
        "# Execute the data pipeline\n",
        "data_pipeline()"
      ],
      "metadata": {
        "id": "7-kLb-PZbcku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenge"
      ],
      "metadata": {
        "id": "O8btojb2nQPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a data pipeline that extracts quotes from the [Quotes to Scrape](https://quotes.toscrape.com) website, transforms them, and saves them to a CSV file, we'll modify the existing pipeline accordingly."
      ],
      "metadata": {
        "id": "XvTjIhyvnR0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract function\n",
        "def extract():\n",
        "    response = requests.get(\"http://books.toscrape.com/\")\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    quotes = []\n",
        "\n",
        "    # Find all quote containers\n",
        "    quote_containers = soup.find_all('div', class_='quote')\n",
        "\n",
        "    # Extract text, author, and tags from each quote container\n",
        "    for container in quote_containers:\n",
        "        text = container.find('span', class_='text').text\n",
        "        author = container.find('small', class_='author').text\n",
        "        tags = container.find('div', class_='tags').find_all('a')\n",
        "        tags = [tag.text for tag in tags]\n",
        "        quotes.append([text, author, tags])\n",
        "\n",
        "    return quotes\n",
        "\n",
        "# Transform function\n",
        "def transform(data):\n",
        "    # Convert the list of quotes into a DataFrame\n",
        "    df = pd.DataFrame(data, columns=['Quote', 'Author', 'Tags'])\n",
        "\n",
        "    # Join tags into a single string separated by commas\n",
        "    df['Tags'] = df['Tags'].apply(lambda x: ', '.join(x))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Load function\n",
        "def load(data):\n",
        "      data.to_csv('quotes_data.csv', index=False)\n",
        "\n",
        "# Create the data pipeline\n",
        "def data_pipeline():\n",
        "    quote_data = extract()\n",
        "    df = transform(quote_data)\n",
        "    load(df)\n",
        "\n",
        "# Execute the data pipeline\n",
        "data_pipeline()"
      ],
      "metadata": {
        "id": "iIhpzlKFnbMw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}